@misc{123dokMetodeElemenHingga,
  title = {{Metode Elemen Hingga - DASAR TEORI DASAR TEORI}},
  author = {{123Dok}},
  urldate = {2024-01-06},
  abstract = {Metode Elemen Hingga - DASAR TEORI DASAR TEORI},
  howpublished = {https://123dok.com/article/metode-elemen-hingga-dasar-teori-dasar-teori.zk81d0ez},
  langid = {indonesian},
  file = {/home/alfarizi/Zotero/storage/A8GPVZ7P/metode-elemen-hingga-dasar-teori-dasar-teori.html}
}

@misc{amerioControlFlow2019,
  title = {Control {{Flow}}},
  author = {Amerio, Aurelio},
  year = {2019},
  month = nov,
  journal = {TechyTok},
  urldate = {2023-12-28},
  abstract = {From zero to Julia Lesson 4. Control Flow},
  howpublished = {https://techytok.com/lesson-control-flow/},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/YK29UITU/lesson-control-flow.html}
}

@misc{amerioFunctions2019,
  title = {Functions},
  author = {Amerio, Aurelio},
  year = {2019},
  month = nov,
  journal = {TechyTok},
  urldate = {2023-12-28},
  abstract = {From zero to Julia Lesson 2. An introduction to functions},
  howpublished = {https://techytok.com/lesson-functions/},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/CAWAGTNC/lesson-functions.html}
}

@misc{amerioWorkingArraysBroadcasting2019,
  title = {Working with Arrays: Broadcasting},
  shorttitle = {Working with Arrays},
  author = {Amerio, Aurelio},
  year = {2019},
  month = nov,
  journal = {TechyTok},
  urldate = {2023-11-01},
  abstract = {From zero to Julia Lesson 5. Working with arrays: broadcasting},
  howpublished = {https://techytok.com/lesson-working-with-arrays/},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/X26HMVIR/lesson-working-with-arrays.html}
}

@misc{armWhatCentralProcessing2023,
  title = {What Is a {{Central Processing Unit}}? {\textendash} {{Arm}}{\textregistered}},
  shorttitle = {What Is a {{Central Processing Unit}}?},
  author = {Arm, Ltd},
  year = {2023},
  month = nov,
  journal = {Arm {\textbar} The Architecture for the Digital World},
  urldate = {2023-11-06},
  abstract = {The central processing unit (CPU) is the primary component of a computer that acts as its ``control center.'' The CPU, also referred to as the ``central'' or ``main'' processor, is a complex set of electronic circuitry that runs the machine's operating system and apps. The CPU interprets, processes and executes instructions, most often from the hardware and software programs running on the device.},
  howpublished = {https://www.arm.com/glossary/cpu},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/NRLSHPXU/cpu.html}
}

@misc{armWhatGraphicsProcessing2023,
  title = {What Is a {{Graphics Processing Unit}} ({{GPU}})?},
  author = {Arm, Ltd},
  year = {2023},
  month = nov,
  journal = {Arm {\textbar} The Architecture for the Digital World},
  urldate = {2023-11-06},
  abstract = {A graphics processing unit (GPU) is an electronic circuit designed specifically for rendering images on a screen.},
  howpublished = {https://www.arm.com/glossary/gpus},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/NWLGXPG9/gpus.html}
}

@misc{bezansonJuliaMicroBenchmarks2023,
  title = {Julia {{Micro-Benchmarks}}},
  author = {Bezanson, Jeff and Karpinski, Stefan and Shah, Viral and Edelman, Alan},
  year = {2023},
  urldate = {2023-11-06},
  abstract = {The official website for the Julia Language. Julia is a language that is fast, dynamic, easy to use, and open source. Click here to learn more.},
  howpublished = {https://julialang.org/benchmarks/},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/B2N64FLH/benchmarks.html}
}

@article{bolzSparseMatrixSolvers2003,
  title = {Sparse {{Matrix Solvers}} on the {{GPU}}: {{Conjugate Gradients}} and {{Multigrid}}},
  author = {Bolz, Jeff and Farmer, Ian and Grinspun, Eitan and Schr{\"o}der, Peter},
  year = {2003},
  abstract = {Many computer graphics applications require high-intensity numerical simulation. We show that such computations can be performed efficiently on the GPU, which we regard as a full function streaming processor with high floating-point performance. We implemented two basic, broadly useful, computational kernels: a sparse matrix conjugate gradient solver and a regular-grid multigrid solver. Realtime applications ranging from mesh smoothing and parameterization to fluid solvers and solid mechanics can greatly benefit from these, evidence our example applications of geometric flow and fluid simulation running on NVIDIA's GeForce FX.},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/QHNLLDS3/Bolz et al. - Sparse Matrix Solvers on the GPU Conjugate Gradie.pdf}
}

@book{boydIntroductionAppliedLinear2018,
  title = {Introduction to {{Applied Linear Algebra}}: {{Vectors}}, {{Matrices}}, and {{Least Squares}}},
  shorttitle = {Introduction to {{Applied Linear Algebra}}},
  author = {Boyd, Stephen and Vandenberghe, Lieven},
  year = {2018},
  month = jun,
  edition = {1},
  publisher = {{Cambridge University Press \& Assessment}},
  doi = {10.1017/9781108583664},
  urldate = {2024-02-01},
  abstract = {This groundbreaking textbook combines straightforward explanations with a wealth of practical examples to offer an innovative approach to teaching linear algebra. Requiring no prior knowledge of the subject, it covers the aspects of linear algebra - vectors, matrices, and least squares - that are needed for engineering applications, discussing examples across data science, machine learning and artificial intelligence, signal and image processing, tomography, navigation, control, and finance. The numerous practical exercises throughout allow students to test their understanding and translate their knowledge into solving real-world problems, with lecture slides, additional computational exercises in Julia and MATLAB{\textregistered}, and data sets accompanying the book online. Suitable for both one-semester and one-quarter courses, as well as self-study, this self-contained text provides beginning students with the foundation they need to progress to more advanced study.},
  isbn = {978-1-108-58366-4 978-1-316-51896-0},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/XRL9PMZQ/Boyd and Vandenberghe - 2018 - Introduction to Applied Linear Algebra Vectors, M.pdf}
}

@inproceedings{buiHeterogeneousComputingRealWorld2021,
  title = {Heterogeneous {{Computing}} and {{The Real-World Applications}}},
  booktitle = {2021 {{IEEE}} 12th {{Annual Ubiquitous Computing}}, {{Electronics}} \& {{Mobile Communication Conference}} ({{UEMCON}})},
  author = {Bui, Viet and Pham, Trung and Nguyen, Huy and Tran Gia, Hoang Nhi and Mohd, Tauheed Khan},
  year = {2021},
  month = dec,
  pages = {0747--0751},
  doi = {10.1109/UEMCON53757.2021.9666740},
  urldate = {2024-01-26},
  abstract = {In recent years, Graphics Processing Units (GPUs) in computational science have effectively driven the computing system to achieve higher computational power in various electronic devices. Combining with multi-core Central Processing Units (CPUs), the inevitable CPU-GPU integration is considered the new high-performance trend of the future. There have been multiple explorations on heterogeneous processing techniques, alongside the design of fused CPU and GPU chips. This survey paper will describe, summarize, and analyze some of the latest research from 2015 to map the techniques that can improve this computation hardware's performance or energy efficiency. We then take a look at the collaborative CPU-GPU approaches with regards to runtime and applicability. We have certainty that this report can provide insights into the knowledge of CPU-GPU applications and help find future opportunities for these processors.},
  keywords = {Computational modeling,Computers,CPU,Deep learning,efficiency,GPU,Graphics processing units,hardware,hardware integration,Market research,Runtime,Transportation},
  file = {/home/alfarizi/Zotero/storage/QZURQNIM/Bui et al_2021_Heterogeneous Computing and The Real-World Applications.pdf;/home/alfarizi/Zotero/storage/4I297RMQ/9666740.html}
}

@misc{businesswireHistoryGPUInception2023,
  title = {The {{History}} of the {{GPU}}: {{From Inception}} to {{AI}}},
  shorttitle = {The {{History}} of the {{GPU}}},
  author = {{Businesswire}},
  year = {2023},
  month = feb,
  journal = {Businesswire},
  urldate = {2023-12-25},
  abstract = {These are the first \& only books to cover the history of the GPU, from its origins in the 1950s to the supercomputers \& smartphones it powers today.},
  howpublished = {https://www.businesswire.com/news/home/20230216005383/en/The-History-of-the-GPU-From-Inception-to-AI},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/WCTGNHFB/The-History-of-the-GPU-From-Inception-to-AI.html}
}

@article{casellaMonteCarloStatistical2004,
  title = {Monte {{Carlo Statistical Methods}}},
  author = {Casella, George},
  year = {2004},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/IRFLIKGU/Casella - Monte Carlo Statistical Methods.pdf}
}

@article{cecconMomentumStrategiesComparison2016,
  title = {Momentum {{Strategies}}: {{{\emph{Comparison}}}}{\emph{ of }}{{{\emph{Programming Language Performance}}}}},
  shorttitle = {Momentum {{Strategies}}},
  author = {Ceccon, Francesco and Thukral, Lovjit and Vergel Eleuterio, Pedro},
  year = {2016},
  month = mar,
  journal = {The Journal of Trading},
  volume = {11},
  number = {2},
  pages = {49--53},
  issn = {1559-3967, 2168-8427},
  doi = {10.3905/jot.2016.11.2.049},
  urldate = {2024-01-26},
  abstract = {Given the increase in the popularity of algorithmic trading resulting from an increase in market participants, more considerations are now required to prototype a profitable trading strategy. Trading strategies, which require optimization of parameters based on linear or nonlinear relationships, cause an increase in complexity, which in turn increases computational run time. We find that C provides the best performance for prototyping quantitative trading strategies; however, it is the most time-consuming to implement. Among the languages that allow for faster development times, the difference between Cython and Julia is relatively small, so choice between them comes down to user preference and other factors. We find Julia to be the standout programming language due to its simplicity and high performance.},
  langid = {english}
}

@article{choiParallelClothSimulation2018,
  title = {Parallel Cloth Simulation with {{GPGPU}}},
  author = {Choi, Young-Hwan and Hong, Min and Choi, Yoo-Joo},
  year = {2018},
  month = nov,
  journal = {Multimedia Tools and Applications},
  volume = {77},
  number = {22},
  pages = {30105--30120},
  issn = {1573-7721},
  doi = {10.1007/s11042-018-6188-x},
  urldate = {2024-01-26},
  abstract = {In a 3D simulation, numerous physically and numerically related calculations are required to represent an object realistically. The existing CPU (central processing unit) technology, however, is incapable of handling such a large computational amount in real time. With the recent hardware-technology advancements, the GPU (graphics processing unit) can be used not only for conventional rendering operations, but also for general-purpose computational functions. In this paper, a mass-spring system for which the CPU and GPU versions are tested under the PC and mobile environments wherein the GPGPU (general-purpose computing on GPUs) is applied is proposed. For this paper, a virtual cloth with a mass-spring system was freely dropped onto a table, and the CPU and GPU performances were compared. The computational GPU performances regarding the PC and mobile devices were improved by 9.41 times and 45.11 times, respectively, compared with the CPU. The proposed GPU mass-spring system was then implemented with an edge-centric algorithm and a node-centric algorithm. The edge-centric algorithm is divided into two parts as follows: one for the spring-force calculation and one for the node-position calculation. These two parts are combined into a single computational process for the node-centric algorithm. For this paper, the computational speeds of the two algorithms were measured. The node-centric algorithm is faster than the edge-centric algorithm under the PC environment, but the edge-centric algorithm is faster under the mobile environment.},
  langid = {english},
  keywords = {GPU-based parallel processing,Mass-spring system,Physically-based simulation,Real-time simulation},
  file = {/home/alfarizi/Zotero/storage/VW54S7WK/Choi et al_2018_Parallel cloth simulation with GPGPU.pdf}
}

@article{dallyEvolutionGraphicsProcessing2021,
  title = {Evolution of the {{Graphics Processing Unit}} ({{GPU}})},
  author = {Dally, William J. and Keckler, Stephen W. and Kirk, David B.},
  year = {2021},
  month = nov,
  journal = {IEEE Micro},
  volume = {41},
  number = {6},
  pages = {42--51},
  issn = {1937-4143},
  doi = {10.1109/MM.2021.3113475},
  urldate = {2023-12-25},
  abstract = {Graphics processing units (GPUs) power today's fastest supercomputers, are the dominant platform for deep learning, and provide the intelligence for devices ranging from self-driving cars to robots and smart cameras. They also generate compelling photorealistic images at real-time frame rates. GPUs have evolved by adding features to support new use cases. NVIDIA's GeForce 256, the first GPU, was a dedicated processor for real-time graphics, an application that demands large amounts of floating-point arithmetic for vertex and fragment shading computations and high memory bandwidth. As real-time graphics advanced, GPUs became programmable. The combination of programmability and floating-point performance made GPUs attractive for running scientific applications. Scientists found ways to use early programmable GPUs by casting their calculations as vertex and fragment shaders. GPUs evolved to meet the needs of scientific users by adding hardware for simpler programming, double-precision floating-point arithmetic, and resilience.},
  file = {/home/alfarizi/Zotero/storage/TU7DY9GY/Dally et al_2021_Evolution of the Graphics Processing Unit (GPU).pdf;/home/alfarizi/Zotero/storage/N4GYR792/9623445.html}
}

@misc{etsworldsPengertianDanKonsep2018,
  title = {Pengertian Dan {{Konsep Dasar Metode Elemen Hingga}} ({{Finite Elemen Methode}})},
  author = {{Etsworlds}},
  year = {2018},
  urldate = {2024-01-06},
  abstract = {Penjelasan tentang konsep dasar dari metode elemen hingga (finite methode element), pengertian dan aplikasi dari metode elemen hingga},
  langid = {british},
  file = {/home/alfarizi/Zotero/storage/DE2Y9LF2/konsep-dasar-metode-elemen-hingga.html}
}

@misc{evansonExplainerWhatAPI2021,
  title = {Explainer: {{What}} Is an {{API}}?},
  shorttitle = {Explainer},
  author = {Evanson, Nick},
  year = {2021},
  month = jun,
  journal = {TechSpot},
  urldate = {2023-11-06},
  abstract = {You've probably heard about APIs and even used them before, but what are they? Let us explain what an API is, and take a quick look at...},
  howpublished = {https://www.techspot.com/article/2237-what-is-api/},
  langid = {american},
  file = {/home/alfarizi/Zotero/storage/5MQQZ83K/2237-what-is-api.html}
}

@misc{FlowchartDiagram,
  title = {Flowchart {{Diagram}}},
  urldate = {2023-12-28},
  howpublished = {https://docs.staruml.io/working-with-additional-diagrams/flowchart-diagram},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/4ZXRZLDN/flowchart-diagram.html}
}

@misc{gigabyteHPCHighPerformance2023,
  title = {{{HPC}} ({{High Performance Computing}}) - {{GIGABYTE Global}}},
  author = {{Gigabyte}},
  year = {2023},
  journal = {GIGABYTE},
  urldate = {2023-11-06},
  abstract = {What is it?				High Performance Computing, or HPC, refers to the ability to process data and perform calculations at high speeds, especially in systems that...},
  howpublished = {https://www.gigabyte.com/Glossary/hpc},
  file = {/home/alfarizi/Zotero/storage/7ND6M4F9/hpc.html}
}

@misc{gigabyteWhatGPGPUWhy2023,
  title = {What Is {{GPGPU}}? {{Why}} Do You Need It? - {{GIGABYTE Global}}},
  shorttitle = {What Is {{GPGPU}}?},
  author = {{Gigabyte}},
  year = {2023},
  journal = {GIGABYTE},
  urldate = {2023-11-06},
  abstract = {What is it?				GPGPU stands for ``general-purpose computing on graphics processing units'', or ``general-purpose graphics processing units'' for short. The idea...},
  howpublished = {https://www.gigabyte.com/Glossary/gpgpu},
  file = {/home/alfarizi/Zotero/storage/UXVZ3948/gpgpu.html}
}

@article{gmysComparativeStudyHighproductivity2020,
  title = {A Comparative Study of High-Productivity High-Performance Programming Languages for Parallel Metaheuristics},
  author = {Gmys, Jan and Carneiro, Tiago and Melab, Nouredine and Talbi, El-Ghazali and Tuyttens, Daniel},
  year = {2020},
  month = sep,
  journal = {Swarm and Evolutionary Computation},
  volume = {57},
  pages = {100720},
  issn = {22106502},
  doi = {10.1016/j.swevo.2020.100720},
  urldate = {2024-01-26},
  abstract = {Semantic Scholar extracted view of "A comparative study of high-productivity high-performance programming languages for parallel metaheuristics" by Jan Gmys et al.},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/HZXEC5XV/Gmys et al_2020_A comparative study of high-productivity high-performance programming languages.pdf}
}

@misc{hagoortExploringGPUArchitecture2023,
  title = {Exploring the {{GPU Architecture}} {\textbar} {{VMware}}},
  author = {Hagoort, Niels},
  year = {2023},
  journal = {The Cloud Platform Tech Zone},
  urldate = {2023-12-26},
  abstract = {A Graphics Processor Unit (GPU) is mostly known for the hardware device used when running applications that weigh heavy on graphics, i.e. 3D modeling software or VDI infrastructures. In the consumer market, a GPU is mostly used to accelerate gaming graphics. Today, GPGPU's (General Purpose GPU) are the choice of hardware to accelerate computational workloads in modern High Performance Computing (HPC) landscapes.},
  howpublished = {https://core.vmware.com/resource/exploring-gpu-architecture},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/RGWWCK8N/exploring-gpu-architecture.html}
}

@article{hanGPUAccelerationComputational,
  title = {{{GPU Acceleration}} for {{Computational Finance}}},
  author = {Han, Chuan-Hsiang},
  abstract = {Recent progress of graphics processing unit (GPU) computing with applications in science and technology has demonstrated tremendous impact over the last decade. However, financial applications by GPU computing are less discussed and may cause an obstacle toward the development of financial technology, an emerging and disruptive field focusing on the efficiency improvement of our current financial system. This paper aims to raise the attention of GPU computing in finance by first empirically investigate the performance of three basic computational methods including solving a linear system, Fast Fourier transform, and Monte Carlo simulation. Then a fast calibration of the wing model to implied volatilities is explored with a set of traded futures and option data in high frequency. At least 60\% executing time reduction on this calibration is obtained under the Matlab computational environment. This finding enables the disclosure of an instant market change so that a real-time surveillance for financial markets can be established for either trading or risk management purpose.},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/8U68Y9XP/Han - GPU Acceleration for Computational Finance.pdf}
}

@inproceedings{harrisSimulationCloudDynamics2005b,
  title = {Simulation of Cloud Dynamics on Graphics Hardware},
  booktitle = {{{ACM SIGGRAPH}} 2005 {{Courses}} on   - {{SIGGRAPH}} '05},
  author = {Harris, Mark J. and Baxter, William V. and Scheuermann, Thorsten and Lastra, Anselmo},
  year = {2005},
  pages = {223},
  publisher = {{ACM Press}},
  address = {{Los Angeles, California}},
  doi = {10.1145/1198555.1198793},
  urldate = {2023-12-10},
  abstract = {This paper presents a physically-based, visually-realistic interactive cloud simulation. Clouds in our system are modeled using partial differential equations describing fluid motion, thermodynamic processes, buoyant forces, and water phase transitions. We also simulate the interaction of clouds with light, including self-shadowing and light scattering.},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/I8A3JF4R/Harris et al. - 2005 - Simulation of cloud dynamics on graphics hardware.pdf}
}

@misc{helenGPUArchitectureStructure2020,
  title = {{{GPU Architecture}}: {{A Structure}} for {{Data Parallel Throughput}} - {{MiniTool Partition Wizard}}},
  shorttitle = {{{GPU Architecture}}},
  author = {{Helen}},
  year = {2020},
  month = sep,
  journal = {MiniTool},
  urldate = {2023-12-26},
  abstract = {This article elaborates on the basic GPU architecture and its functions. A large number of cores enables the parallel processing of data throughput.},
  chapter = {Partition Magic},
  howpublished = {https://www.partitionwizard.com/partitionmagic/gpu-architecture.html},
  langid = {american},
  file = {/home/alfarizi/Zotero/storage/N9G8U6QY/gpu-architecture.html}
}

@inproceedings{hunoldBenchmarkingJuliaCommunication2020,
  title = {Benchmarking {{Julia}}'s {{Communication Performance}}: {{Is Julia HPC}} Ready or {{Full HPC}}?},
  shorttitle = {Benchmarking {{Julia}}'s {{Communication Performance}}},
  booktitle = {2020 {{IEEE}}/{{ACM Performance Modeling}}, {{Benchmarking}} and {{Simulation}} of {{High Performance Computer Systems}} ({{PMBS}})},
  author = {Hunold, Sascha and Steiner, Sebastian},
  year = {2020},
  month = nov,
  pages = {20--25},
  doi = {10.1109/PMBS51919.2020.00008},
  urldate = {2023-12-10},
  abstract = {Julia has quickly become one of the main programming languages for computational sciences, mainly due to its speed and flexibility. The speed and efficiency of Julia are the main reasons why researchers in the field of High Performance Computing have started porting their applications to Julia.Since Julia has a very small binding-overhead to C, many efficient computational kernels can be integrated into Julia without any noticeable performance drop. For that reason, highly tuned libraries, such as the Intel MKL or OpenBLAS, will allow Julia applications to achieve similar computational performance as their C counterparts. Yet, two questions remain: 1) How fast is Julia for memory-bound applications? 2) How efficient can MPI functions be called from a Julia application?In this paper, we will assess the performance of Julia with respect to HPC. To that end, we examine the raw throughput achievable with Julia using a new Julia port of the well-known STREAM benchmark. We also compare the running times of the most commonly used MPI collective operations (e.g., MPI\_Allreduce) with their C counterparts. Our analysis shows that HPC performance of Julia is on-par with C in the majority of cases.},
  file = {/home/alfarizi/Zotero/storage/SXKVFG9D/Hunold_Steiner_2020_Benchmarking Julia’s Communication Performance.pdf;/home/alfarizi/Zotero/storage/NVP7KNBF/9307882.html}
}

@phdthesis{ichrossofilmubarotPerancanganKonstruksiMesin2017,
  title = {{Perancangan Konstruksi Mesin Press Panas Pneumatik Berbasis 2 Kontrol Relay Dengan Bantuan Software Solidwork}},
  author = {{Ichros Sofil Mubarot}},
  year = {2017},
  address = {{Surabaya}},
  urldate = {2024-01-06},
  abstract = {Dari  hasil  simulasi  software  solidwork  dan  verifikasi  perhitungan  manual,  didapatkan  nilai  tegangan  maksimum  yang  terjadi  pada  bagian  komponen  paling  kritis  yaitu,},
  langid = {indonesian},
  school = {Institut Teknologi Sepuluh November},
  file = {/home/alfarizi/Zotero/storage/CPVCZGBZ/zk81d0ez-perancangan-konstruksi-pneumatik-berbasis-kontrol-software-solidwork-repository.html}
}

@misc{intelCPUVsGPU2023,
  title = {{{CPU}} vs. {{GPU}}: {{What}}'s the {{Difference}}?},
  shorttitle = {{{CPU}} vs. {{GPU}}},
  author = {{Intel}},
  year = {2023},
  journal = {Intel},
  urldate = {2023-11-06},
  abstract = {Learn about the CPU vs GPU difference, explore uses and the architecture benefits, and their roles for accelerating deep-learning and AI.},
  howpublished = {https://www.intel.com/content/www/us/en/products/docs/processors/cpu-vs-gpu.html},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/C4SSNAF4/cpu-vs-gpu.html}
}

@misc{ismiPentingUntukData2021,
  title = {Penting Untuk {{Data Science}}, Yuk, {{Kenalan}} Dengan {{Bahasa Pemrograman Julia}}},
  author = {Ismi, Trias},
  year = {2021},
  month = aug,
  journal = {Glints Blog},
  urldate = {2023-12-26},
  abstract = {Kamu tertarik menggeluti bidang data science? Sebaiknya ketahui dulu apa itu bahasa pemrograman Julia dan manfaatnya di sini!},
  langid = {american},
  file = {/home/alfarizi/Zotero/storage/YY62K2R7/bahasa-pemrograman-julia.html}
}

@book{kalosMonteCarloMethods2008,
  title = {Monte {{Carlo Methods}}},
  author = {Kalos, Malvin H. and Whitlock, Paula A.},
  year = {2008},
  month = oct,
  publisher = {{John Wiley \& Sons}},
  abstract = {This introduction to Monte Carlo methods seeks to identify and study the unifying elements that underlie their effective application. Initial chapters provide a short treatment of the probability and statistics needed as background, enabling those without experience in Monte Carlo techniques to apply these ideas to their research. The book focuses on two basic themes: The first is the importance of random walks as they occur both in natural stochastic systems and in their relationship to integral and differential equations. The second theme is that of variance reduction in general and importance sampling in particular as a technique for efficient use of the methods. Random walks are introduced with an elementary example in which the modeling of radiation transport arises directly from a schematic probabilistic description of the interaction of radiation with matter. Building on this example, the relationship between random walks and integral equations is outlined. The applicability of these ideas to other problems is shown by a clear and elementary introduction to the solution of the Schrodinger equation by random walks. The text includes sample problems that readers can solve by themselves to illustrate the content of each chapter.  This is the second, completely revised and extended edition of the successful monograph, which brings the treatment up to date and incorporates the many advances in Monte Carlo techniques and their applications, while retaining the original elementary but general approach.},
  googlebooks = {b8Xb4rBkygUC},
  isbn = {978-3-527-40760-6},
  langid = {english},
  keywords = {Science / Physics / General,Science / Physics / Mathematical \& Computational},
  file = {/home/alfarizi/Zotero/storage/3PC9ETSP/Monte Carlo Methods - Second Revised and Enlarged .pdf}
}

@article{khairySurveyArchitecturalApproaches2019,
  title = {A Survey of Architectural Approaches for Improving {{GPGPU}} Performance, Programmability and Heterogeneity},
  author = {Khairy, Mahmoud and Wassal, Amr G. and Zahran, Mohamed},
  year = {2019},
  month = may,
  journal = {Journal of Parallel and Distributed Computing},
  volume = {127},
  pages = {65--88},
  issn = {0743-7315},
  doi = {10.1016/j.jpdc.2018.11.012},
  urldate = {2023-11-06},
  abstract = {With the skyrocketing advances of process technology, the increased need to process huge amount of data, and the pivotal need for power efficiency, the usage of Graphics Processing Units (GPUs) for General Purpose Computing becomes a trend and natural. GPUs have high computational power and excellent performance per watt, for data parallel applications, relative to traditional multicore processors. GPUs appear as discrete or embedded with Central Processing Units (CPUs), leading to a scheme of heterogeneous computing. Heterogeneous computing brings as many challenges as it brings opportunities. To get the most of such systems, we need to guarantee high GPU utilization, deal with irregular control flow of some workloads, and struggle with far-friendly-programming models. The aim of this paper is to provide a survey about GPUs from two perspectives: (1) architectural advances to improve performance and programmability and (2) advances to enhance CPU{\textendash}GPU integration in heterogeneous systems. This will help researchers see the opportunities and challenges of using GPUs for general purpose computing, especially in the era of big data and the continuous need of high-performance computing.},
  keywords = {Control divergence,GPGPU,Heterogeneous architecture,Memory systems},
  file = {/home/alfarizi/Zotero/storage/X2XJHR56/S0743731518308669.html}
}

@article{kipferUberFlowGPUBasedParticle2004,
  title = {{{UberFlow}}: {{A GPU-Based Particle Engine}}},
  author = {Kipfer, Peter and Segal, Mark and Westermann, Rudiger},
  year = {2004},
  abstract = {We present a system for real-time animation of large particle sets using GPU computation and memory objects in OpenGL. Our system implements a versatile particle engine, including inter-particle collisions and visibility sorting. By combining memory objects with floating-point fragment programs, we have implemented a particle engine that entirely avoids the transfer of particle data at runtime.},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/DEHPWA7D/Kipfer et al. - UberFlow A GPU-Based Particle Engine.pdf}
}

@book{kolbHardwarebasedSimulationCollision2004,
  title = {Hardware-Based {{Simulation}} and {{Collision Detection}} for {{Large Particle Systems}}},
  author = {Kolb, A. and Latta, L. and {Rezk-Salama}, C.},
  year = {2004},
  publisher = {{The Eurographics Association}},
  issn = {1727-3471},
  doi = {10.2312/EGGH/EGGH04/123-132},
  urldate = {2023-12-10},
  abstract = {Particle systems have long been recognized as an essential building block for detail-rich and lively visual environments. Current implementations can handle up to 10,000 particles in real-time simulations and are mostly limited by the transfer of particle data from the main processor to the graphics hardware (GPU) for rendering. This paper introduces a full GPU implementation using fragment shaders of both the simulation and rendering of a dynamically-growing particle system. Such an implementation can render up to 1 million particles in real-time on recent hardware. The massively parallel simulation handles collision detection and reaction of particles with objects for arbitrary shape. The collision detection is based on depth maps that represent the outer shape of an object. The depth maps store distance values and normal vectors for collision reaction. Using a special texturebased indexing technique to represent normal vectors, standard 8-bit textures can be used to describe the complete depth map data. Alternately, several depth maps can be stored in one floating point texture. In addition, a GPU-based parallel sorting algorithm is introduced that can be used to perform a depth sorting of the particles for correct alpha blending.},
  isbn = {978-3-905673-15-9},
  langid = {english},
  annotation = {Accepted: 2013-10-28T10:02:18Z},
  file = {/home/alfarizi/Zotero/storage/Y6UAMFCL/Kolb et al_2004_Hardware-based Simulation and Collision Detection for Large Particle Systems.pdf}
}

@incollection{kukunasChapterIntelPentium2015,
  title = {Chapter 2 - {{Intel}}{\textregistered} {{Pentium}}{\textregistered} {{Processors}}},
  booktitle = {Power and {{Performance}}},
  author = {Kukunas, Jim},
  year = {2015},
  month = jan,
  pages = {31--41},
  publisher = {{Morgan Kaufmann}},
  address = {{Boston}},
  doi = {10.1016/B978-0-12-800726-6.00002-1},
  urldate = {2023-11-06},
  abstract = {This chapter builds on the previous chapter by exploring the Intel(R) Pentium(R) processor family. The chapter begins by looking at the changes introduced by the Intel(R) Pentium(R), Intel(R) Pentium(R) Pro, and Intel(R) Pentium(R) 4 processors. Topics covered include the superscalar execution pipeline, out-of-order execution, {$\mu$}ops, and Intel(R) Hyper-Threading. The chapter ends with the extension from 32- to 64-bit processors.},
  isbn = {978-0-12-800726-6},
  keywords = {Hyper-Threading,i686,Out-of-order execution,Pentium(R),Pentium(R) 4,Pentium(R) II,Pentium(R) III,Pentium(R) Pro,superscalar},
  file = {/home/alfarizi/Zotero/storage/JQUGDIPZ/B9780128007266000021.html}
}

@article{kulyabovComputerAlgebraJULIA2021,
  title = {Computer {{Algebra}} in {{JULIA}}},
  author = {Kulyabov, D. S. and Korol'kova, A. V.},
  year = {2021},
  month = mar,
  journal = {Programming and Computer Software},
  volume = {47},
  number = {2},
  pages = {133--138},
  issn = {1608-3261},
  doi = {10.1134/S0361768821020079},
  urldate = {2024-01-26},
  abstract = {Recently, the place of the main programming language for scientific and engineering computations has been little by little taken by Julia. Some users want to work completely within the Julia ecosystem like they work within the Python ecosystem. There are libraries for Julia that cover the majority of scientific and engineering computations demands. The aim of this paper is to combine the usage of Julia framework for numerical computations and for symbolic computations in mathematical modeling problems. The main functional domains determining various variants of the application of computer algebra systems are described. In each of these domains, generic representatives of computer algebra systems in Julia are distinguished. The conclusion is that it is possible (and even convenient) to use computer algebra systems within the Julia ecosystem.},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/6A59SZVN/Kulyabov_Korol’kova_2021_Computer Algebra in JULIA.pdf}
}

@misc{learningUnderstandingArchitectureGPU2023,
  title = {Understanding the Architecture of a {{GPU}}},
  author = {Learning, Vitality},
  year = {2023},
  month = sep,
  journal = {CodeX},
  urldate = {2024-01-07},
  abstract = {Recently, in the story The evolution of a GPU: from gaming to computing, the hystorical evolution of CPUs and GPUs has been discussed and{\dots}},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/XEMC7Q5Y/understanding-the-architecture-of-a-gpu-d5d2d2e8978b.html}
}

@article{mouraUsageJuliaProgramming2019,
  title = {The {{Usage}} of {{Julia Programming}} in {{Grounding Grids Simulations}} : {{An}} Alternative to {{MATLAB}} and {{Python}}},
  shorttitle = {The {{Usage}} of {{Julia Programming}} in {{Grounding Grids Simulations}}},
  author = {Moura, Rodolfo A. R. and Schroeder, Marco A. O. and Silva, Samuel J. S. and Nepomuceno, Erivelton G. and Vieira, Pedro H. N. and Lima, Antonio C. S.},
  year = {2019},
  month = sep,
  journal = {2019 International Symposium on Lightning Protection (XV SIPDA)},
  pages = {1--4},
  publisher = {{IEEE}},
  address = {{S{\~a}o Paulo, Brazil}},
  doi = {10.1109/SIPDA47030.2019.8951702},
  urldate = {2024-01-26},
  abstract = {Matlab and Python have been widely used in science field, mainly used by scientists who are involved in numerical and technical computing. However, other programming languages have arisen in recent years aim to create a combination of power, and efficiency for free. In light of this, this work presents a comparison between three computational environments (MATLAB, Python and Julia) {\textendash} on the study of grounding grids for lightning transients {\textendash} to evaluate the best programming language (among all studied). The main parameter presented on this paper is computational time. The results showed Julia programming language provided simulations six times faster than Matlab and twenty-eight faster than Python.},
  isbn = {9781728118918},
  file = {/home/alfarizi/Zotero/storage/WDITVYVS/Moura et al_2019_The Usage of Julia Programming in Grounding Grids Simulations.pdf}
}

@misc{oanceaGPGPUComputing2014,
  title = {{{GPGPU Computing}}},
  author = {Oancea, Bogdan and Andrei, Tudorel and Dragoescu, Raluca Mariana},
  year = {2014},
  month = aug,
  number = {arXiv:1408.6923},
  eprint = {1408.6923},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-12-09},
  abstract = {Since the first idea of using GPU to general purpose computing, things have evolved over the years and now there are several approaches to GPU programming. GPU computing practically began with the introduction of CUDA (Compute Unified Device Architecture) by NVIDIA and Stream by AMD. These are APIs designed by the GPU vendors to be used together with the hardware that they provide. A new emerging standard, OpenCL (Open Computing Language) tries to unify different GPU general computing API implementations and provides a framework for writing programs executed across heterogeneous platforms consisting of both CPUs and GPUs. OpenCL provides parallel computing using task-based and data-based parallelism. In this paper we will focus on the CUDA parallel computing architecture and programming model introduced by NVIDIA. We will present the benefits of the CUDA programming model. We will also compare the two main approaches, CUDA and AMD APP (STREAM) and the new framwork, OpenCL that tries to unify the GPGPU computing models.},
  archiveprefix = {arxiv},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing}},
  file = {/home/alfarizi/Zotero/storage/IRAH4M89/Oancea et al_2014_GPGPU Computing.pdf;/home/alfarizi/Zotero/storage/SFLKS67H/1408.html}
}

@article{owensSurveyGeneralPurpose2007,
  title = {A {{Survey}} of {{General}}-{{Purpose Computation}} on {{Graphics Hardware}}},
  author = {Owens, John D. and Luebke, David and Govindaraju, Naga and Harris, Mark and Kr{\"u}ger, Jens and Lefohn, Aaron E. and Purcell, Timothy J.},
  year = {2007},
  month = mar,
  journal = {Computer Graphics Forum},
  volume = {26},
  number = {1},
  pages = {80--113},
  issn = {0167-7055, 1467-8659},
  doi = {10.1111/j.1467-8659.2007.01012.x},
  urldate = {2023-12-10},
  abstract = {The rapid increase in the performance of graphics hardware, coupled with recent improvements in its programmability, have made graphics hardware a compelling platform for computationally demanding tasks in a wide variety of application domains. In this report, we describe, summarize, and analyze the latest research in mapping general-purpose computation to graphics hardware.},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/A4N5D7IQ/Owens et al. - 2007 - A Survey of General‐Purpose Computation on Graphic.pdf}
}

@book{pharrGPUGemsProgramming2005,
  title = {{{GPU Gems}} 2: {{Programming Techniques}} for {{High-Performance Graphics}} and {{General-Purpose Computation}} ({{Gpu Gems}})},
  shorttitle = {{{GPU Gems}} 2},
  author = {Pharr, Matt and Fernando, Randima},
  year = {2005},
  month = feb,
  publisher = {{Addison-Wesley Professional}},
  isbn = {978-0-321-33559-3},
  file = {/home/alfarizi/Zotero/storage/27TCH4MC/Pharr and Fernando - 2005 - GPU Gems 2 Programming Techniques for High-Perfor.html}
}

@article{ravasiLeveragingGPUsMatrixfree2021,
  title = {Leveraging {{GPUs}} for Matrix-Free Optimization with {{PyLops}}},
  author = {Ravasi, M.},
  year = {2021},
  journal = {Fifth EAGE Workshop on High Performance Computing for Upstream},
  pages = {1--5},
  publisher = {{European Association of Geoscientists \& Engineers}},
  address = {{Online,}},
  doi = {10.3997/2214-4609.2021612003},
  urldate = {2024-01-26},
  abstract = {Summary The use of Graphics Processing Units (GPUs) for scientific computing has become mainstream in the last decade. Applications ranging from deep learning to seismic modelling have benefitted from the increase in computational efficiency compared to their equivalent CPU-based implementations. Since many inverse problems in geophysics relies on similar core computations {\textendash} e.g. dense linear algebra operations, convolutions, FFTs {\textendash} it is reasonable to expect similar performance gains if GPUs are also leveraged in this context. In this paper we discuss how we have been able to take PyLops, a Python library for matrix-free linear algebra and optimization originally developed for singe-node CPUs, and create a fully compatible GPU backend with the help of CuPy and cuSignal. A benchmark suite of our core operators shows that an average 65x speed-up in computations can be achieved when running computations on a V100 GPU. Moreover, by careful modification of the inner working of the library, end users can obtain such a performance gain at virtually no cost: minimal code changes are required when switching between the CPU and GPU backends, mostly consisting of moving the data vector to the GPU device prior to solving an inverse problem with one of PyLops' solvers.},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/E3BVW3LC/Ravasi_2021_Leveraging GPUs for matrix-free optimization with PyLops.pdf}
}

@article{thakurTextSummarizerUsing2022,
  title = {Text {{Summarizer Using Julia}}},
  author = {Thakur, Amey},
  year = {2022},
  month = jan,
  journal = {International Journal for Research in Applied Science and Engineering Technology},
  volume = {10},
  number = {1},
  pages = {1371--1375},
  issn = {23219653},
  doi = {10.22214/ijraset.2022.40066},
  urldate = {2024-01-26},
  abstract = {Abstract: The purpose of this paper is to introduce the Julia programming language with a concentration on Text Summarization. An extractive summarization algorithm is used for summarizing. Julia's evolution and features, as well as comparisons to other programming languages, are briefly discussed. The system's operation is depicted in a flow diagram, which illustrates the processes of sentence selection. Keywords: Text Summarizer, Extractive Summarization, Sentence Score, Topic Representation, Julia Programming Language},
  file = {/home/alfarizi/Zotero/storage/2CB4VN7U/Thakur_2022_Text Summarizer Using Julia.pdf}
}

@article{tjandraParallelNumericalComputation2022,
  title = {Parallel {{Numerical Computation}}: A {{Comparative Study}} on {{Cpu-gpu Performance}} in {{Pi Digits Computation}}},
  shorttitle = {Parallel {{Numerical Computation}}},
  author = {Tjandra, Yozef and Lawalat, Sanga},
  year = {2022},
  month = sep,
  journal = {Jurnal Pilar Nusa Mandiri},
  volume = {18},
  number = {2},
  pages = {93--100},
  issn = {2527-6514, 1978-1946},
  doi = {10.33480/pilar.v18i2.3291},
  urldate = {2024-01-26},
  abstract = {As the usage of GPU (Graphical Processing Unit) for non-graphical computation is rising, one important area is to study how the device helps improve numerical calculations. In this work, we present a time performance comparison between purely CPU (serial) and GPU-assisted (parallel) programs in numerical computation. Specifically, we design and implement the calculation of the hexadecimal -digit of the irrational number Pi in two ways: serial and parallel. Both programs are based upon the BBP formula for Pi in the form of infinite series identity. We then provide a detailed time performance analysis of both programs based on the magnitude. Our result shows that the GPU-assisted parallel algorithm ran a hundred times faster than the serial algorithm. To be more precise, we offer that as the value~ grows, the ratio between the execution time of the serial and parallel algorithms also increases. Moreover, when~ it is large enough, that is This GPU efficiency ratio converges to a constant, showing the GPU's maximally utilized capacity. On the other hand, for sufficiently small enough, the serial algorithm performed solely on the CPU works faster since the GPU's small usage of parallelism does not help much compared to the arithmetic complexity.},
  file = {/home/alfarizi/Zotero/storage/AAUND48W/Tjandra_Lawalat_2022_PARALLEL NUMERICAL COMPUTATION.pdf}
}

@article{tomasiNewSolutionsScientific2018,
  title = {Towards New Solutions for Scientific Computing: The Case of {{Julia}}},
  shorttitle = {Towards New Solutions for Scientific Computing},
  author = {Tomasi, M. and Giordano, Mos{\`e}},
  year = {2018},
  month = dec,
  journal = {ArXiv},
  urldate = {2024-01-26},
  abstract = {This year marks the consolidation of Julia (this https URL), a programming language designed for scientific computing, as the first stable version (1.0) has been released, in August 2018. Among its main features, expressiveness and high execution speeds are the most prominent: the performance of Julia code is similar to statically compiled languages, yet Julia provides a nice interactive shell and fully supports Jupyter; moreover, it can transparently call external codes written in C, Fortran, and even Python and R without the need of wrappers. The usage of Julia in the astronomical community is growing, and a GitHub organization named JuliaAstro takes care of coordinating the development of packages. In this paper, we present the features and shortcomings of this language and discuss its application in astronomy and astrophysics.},
  file = {/home/alfarizi/Zotero/storage/CH7A5G6J/Tomasi_Giordano_2018_Towards new solutions for scientific computing.pdf}
}

@article{wikipediaGeneralpurposeComputingGraphics2023,
  title = {General-Purpose Computing on Graphics Processing Units},
  author = {{Wikipedia}},
  year = {2023},
  month = dec,
  journal = {Wikipedia},
  urldate = {2023-12-25},
  abstract = {General-purpose computing on graphics processing units (GPGPU, or less often GPGP) is the use of a graphics processing unit (GPU), which typically handles computation only for computer graphics, to perform computation in applications traditionally handled by the central processing unit (CPU). The use of multiple video cards in one computer, or large numbers of graphics chips, further parallelizes the already parallel nature of graphics processing.Essentially, a GPGPU pipeline is a kind of parallel processing between one or more GPUs and CPUs that analyzes data as if it were in image or other graphic form. While GPUs operate at lower frequencies, they typically have many times the number of cores. Thus, GPUs can process far more pictures and graphical data per second than a traditional CPU. Migrating data into graphical form and then using the GPU to scan and analyze it can create a large speedup. GPGPU pipelines were developed at the beginning of the 21st century for graphics processing (e.g. for better shaders). These pipelines were found to fit scientific computing needs well, and have since been developed in this direction.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1189744303},
  file = {/home/alfarizi/Zotero/storage/VWABBG8V/General-purpose_computing_on_graphics_processing_units.html}
}

@article{wikipediaGraphicsProcessingUnit2023,
  title = {Graphics Processing Unit},
  author = {{Wikipedia}},
  year = {2023},
  month = dec,
  journal = {Wikipedia},
  urldate = {2023-12-25},
  abstract = {A graphics processing unit (GPU) is a specialized electronic circuit initially designed to accelerate computer graphics and image processing (either on a video card or embedded on motherboards, mobile phones, personal computers, workstations, and game consoles). After their initial design, GPUs were found to be useful for non-graphic calculations involving embarrassingly parallel problems due to their parallel structure. Other non-graphical uses include the training of neural networks and cryptocurrency mining.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1191578522},
  file = {/home/alfarizi/Zotero/storage/MVA3VFDE/Graphics_processing_unit.html}
}

@article{xiaoJuliaLanguageComputational2022b,
  title = {Julia {{Language}} in {{Computational Mechanics}}: {{A New Competitor}}},
  shorttitle = {Julia {{Language}} in {{Computational Mechanics}}},
  author = {Xiao, Lei and Mei, Gang and Xi, Ning and Piccialli, Francesco},
  year = {2022},
  month = may,
  journal = {Archives of Computational Methods in Engineering},
  volume = {29},
  number = {3},
  pages = {1713--1726},
  issn = {1886-1784},
  doi = {10.1007/s11831-021-09636-0},
  urldate = {2024-01-26},
  abstract = {Numerical methods are the most popular tools in computational mechanics and have been used to tackle various practical engineering problems. However, the most common programming languages used for implementing numerical methods do not effectively balance the demands of productivity and efficiency. To address the most computationally intensive areas of numerical computing with the increased abstraction and productivity provided by a high-level language, the Julia language was released by the Massachusetts Institute of Technology (MIT) in 2012. The Julia language is an open-source programming language that presents simple syntax and satisfactory performance; this is particularly useful for scientific computing. In this paper, we present a comprehensive survey on the use of the Julia language in computational mechanics. First, we introduce the existing numerical computing packages developed in the Julia language and their relevant applications. Second, we analyze the capabilities of the Julia language in the development of software packages for computational mechanics. Finally, we discuss the open issues regarding the Julia language and the challenges faced when using the Julia language in computational mechanics.},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/9H9PG5JW/Xiao et al_2022_Julia Language in Computational Mechanics.pdf}
}

@article{zappanardelliJuliaSubtypingRational2018,
  title = {Julia Subtyping: A Rational Reconstruction},
  shorttitle = {Julia Subtyping},
  author = {Zappa Nardelli, Francesco and Belyakova, Julia and Pelenitsyn, Artem and Chung, Benjamin and Bezanson, Jeff and Vitek, Jan},
  year = {2018},
  month = oct,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {2},
  number = {OOPSLA},
  pages = {1--27},
  issn = {2475-1421},
  doi = {10.1145/3276483},
  urldate = {2024-01-26},
  abstract = {Programming languages that support multiple dispatch rely on an expressive notion of subtyping to specify method applicability. In these languages, type annotations on method declarations are used to select, out of a potentially large set of methods, the one that is most appropriate for a particular tuple of arguments. Julia is a language for scientific computing built around multiple dispatch and an expressive subtyping relation. This paper provides the first formal definition of Julia's subtype relation and motivates its design. We validate our specification empirically with an implementation of our definition that we compare against the existing Julia implementation on a collection of real-world programs. Our subtype implementation differs on 122 subtype tests out of 6,014,476. The first 120 differences are due to a bug in Julia that was fixed once reported; the remaining 2 are under discussion.},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/68PYVRII/Zappa Nardelli et al_2018_Julia subtyping.pdf}
}
