@misc{amerioWorkingArraysBroadcasting2019,
  title = {Working with Arrays: Broadcasting},
  shorttitle = {Working with Arrays},
  author = {Amerio, Aurelio},
  year = {2019},
  month = nov,
  journal = {TechyTok},
  urldate = {2023-11-01},
  abstract = {From zero to Julia Lesson 5. Working with arrays: broadcasting},
  howpublished = {https://techytok.com/lesson-working-with-arrays/},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/X26HMVIR/lesson-working-with-arrays.html}
}

@misc{armWhatCentralProcessing2023,
  title = {What Is a {{Central Processing Unit}}? {\textendash} {{Arm}}{\textregistered}},
  shorttitle = {What Is a {{Central Processing Unit}}?},
  author = {Arm, Ltd},
  year = {2023},
  month = nov,
  journal = {Arm | The Architecture for the Digital World},
  urldate = {2023-11-06},
  abstract = {The central processing unit (CPU) is the primary component of a computer that acts as its ``control center.'' The CPU, also referred to as the ``central'' or ``main'' processor, is a complex set of electronic circuitry that runs the machine's operating system and apps. The CPU interprets, processes and executes instructions, most often from the hardware and software programs running on the device.},
  howpublished = {https://www.arm.com/glossary/cpu},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/NRLSHPXU/cpu.html}
}

@misc{armWhatGraphicsProcessing2023,
  title = {What Is a {{Graphics Processing Unit}} ({{GPU}})?},
  author = {Arm, Ltd},
  year = {2023},
  month = nov,
  journal = {Arm | The Architecture for the Digital World},
  urldate = {2023-11-06},
  abstract = {A graphics processing unit (GPU) is an electronic circuit designed specifically for rendering images on a screen.},
  howpublished = {https://www.arm.com/glossary/gpus},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/NWLGXPG9/gpus.html}
}

@misc{bezansonJuliaMicroBenchmarks2023,
  title = {Julia {{Micro-Benchmarks}}},
  author = {Bezanson, Jeff and Karpinski, Stefan and Shah, Viral and Edelman, Alan},
  year = {2023},
  urldate = {2023-11-06},
  abstract = {The official website for the Julia Language. Julia is a language that is fast, dynamic, easy to use, and open source. Click here to learn more.},
  howpublished = {https://julialang.org/benchmarks/},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/B2N64FLH/benchmarks.html}
}

@article{bolzSparseMatrixSolvers2003,
  title = {Sparse {{Matrix Solvers}} on the {{GPU}}: {{Conjugate Gradients}} and {{Multigrid}}},
  author = {Bolz, Jeff and Farmer, Ian and Grinspun, Eitan and Schr{\"o}der, Peter},
  year = {2003},
  abstract = {Many computer graphics applications require high-intensity numerical simulation. We show that such computations can be performed efficiently on the GPU, which we regard as a full function streaming processor with high floating-point performance. We implemented two basic, broadly useful, computational kernels: a sparse matrix conjugate gradient solver and a regular-grid multigrid solver. Realtime applications ranging from mesh smoothing and parameterization to fluid solvers and solid mechanics can greatly benefit from these, evidence our example applications of geometric flow and fluid simulation running on NVIDIA's GeForce FX.},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/QHNLLDS3/Bolz et al. - Sparse Matrix Solvers on the GPU Conjugate Gradie.pdf}
}

@misc{evansonExplainerWhatAPI2021,
  title = {Explainer: {{What}} Is an {{API}}?},
  shorttitle = {Explainer},
  author = {Evanson, Nick},
  year = {2021},
  month = jun,
  journal = {TechSpot},
  urldate = {2023-11-06},
  abstract = {You've probably heard about APIs and even used them before, but what are they? Let us explain what an API is, and take a quick look at...},
  howpublished = {https://www.techspot.com/article/2237-what-is-api/},
  langid = {american},
  file = {/home/alfarizi/Zotero/storage/5MQQZ83K/2237-what-is-api.html}
}

@misc{gigabyteHPCHighPerformance2023,
  title = {{{HPC}} ({{High Performance Computing}}) - {{GIGABYTE Global}}},
  author = {{Gigabyte}},
  year = {2023},
  journal = {GIGABYTE},
  urldate = {2023-11-06},
  abstract = {What is it?				High Performance Computing, or HPC, refers to the ability to process data and perform calculations at high speeds, especially in systems that...},
  howpublished = {https://www.gigabyte.com/Glossary/hpc},
  file = {/home/alfarizi/Zotero/storage/7ND6M4F9/hpc.html}
}

@misc{gigabyteWhatGPGPUWhy2023,
  title = {What Is {{GPGPU}}? {{Why}} Do You Need It? - {{GIGABYTE Global}}},
  shorttitle = {What Is {{GPGPU}}?},
  author = {{Gigabyte}},
  year = {2023},
  journal = {GIGABYTE},
  urldate = {2023-11-06},
  abstract = {What is it?				GPGPU stands for ``general-purpose computing on graphics processing units'', or ``general-purpose graphics processing units'' for short. The idea...},
  howpublished = {https://www.gigabyte.com/Glossary/gpgpu},
  file = {/home/alfarizi/Zotero/storage/UXVZ3948/gpgpu.html}
}

@article{hanGPUAccelerationComputational,
  title = {{{GPU Acceleration}} for {{Computational Finance}}},
  author = {Han, Chuan-Hsiang},
  abstract = {Recent progress of graphics processing unit (GPU) computing with applications in science and technology has demonstrated tremendous impact over the last decade. However, financial applications by GPU computing are less discussed and may cause an obstacle toward the development of financial technology, an emerging and disruptive field focusing on the efficiency improvement of our current financial system. This paper aims to raise the attention of GPU computing in finance by first empirically investigate the performance of three basic computational methods including solving a linear system, Fast Fourier transform, and Monte Carlo simulation. Then a fast calibration of the wing model to implied volatilities is explored with a set of traded futures and option data in high frequency. At least 60\% executing time reduction on this calibration is obtained under the Matlab computational environment. This finding enables the disclosure of an instant market change so that a real-time surveillance for financial markets can be established for either trading or risk management purpose.},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/8U68Y9XP/Han - GPU Acceleration for Computational Finance.pdf}
}

@inproceedings{harrisSimulationCloudDynamics2005b,
  title = {Simulation of Cloud Dynamics on Graphics Hardware},
  booktitle = {{{ACM SIGGRAPH}} 2005 {{Courses}} on   - {{SIGGRAPH}} '05},
  author = {Harris, Mark J. and Baxter, William V. and Scheuermann, Thorsten and Lastra, Anselmo},
  year = {2005},
  pages = {223},
  publisher = {{ACM Press}},
  address = {{Los Angeles, California}},
  doi = {10.1145/1198555.1198793},
  urldate = {2023-12-10},
  abstract = {This paper presents a physically-based, visually-realistic interactive cloud simulation. Clouds in our system are modeled using partial differential equations describing fluid motion, thermodynamic processes, buoyant forces, and water phase transitions. We also simulate the interaction of clouds with light, including self-shadowing and light scattering.},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/I8A3JF4R/Harris et al. - 2005 - Simulation of cloud dynamics on graphics hardware.pdf}
}

@inproceedings{hunoldBenchmarkingJuliaCommunication2020,
  title = {Benchmarking {{Julia}}'s {{Communication Performance}}: {{Is Julia HPC}} Ready or {{Full HPC}}?},
  shorttitle = {Benchmarking {{Julia}}'s {{Communication Performance}}},
  booktitle = {2020 {{IEEE}}/{{ACM Performance Modeling}}, {{Benchmarking}} and {{Simulation}} of {{High Performance Computer Systems}} ({{PMBS}})},
  author = {Hunold, Sascha and Steiner, Sebastian},
  year = {2020},
  month = nov,
  pages = {20--25},
  doi = {10.1109/PMBS51919.2020.00008},
  urldate = {2023-12-10},
  abstract = {Julia has quickly become one of the main programming languages for computational sciences, mainly due to its speed and flexibility. The speed and efficiency of Julia are the main reasons why researchers in the field of High Performance Computing have started porting their applications to Julia.Since Julia has a very small binding-overhead to C, many efficient computational kernels can be integrated into Julia without any noticeable performance drop. For that reason, highly tuned libraries, such as the Intel MKL or OpenBLAS, will allow Julia applications to achieve similar computational performance as their C counterparts. Yet, two questions remain: 1) How fast is Julia for memory-bound applications? 2) How efficient can MPI functions be called from a Julia application?In this paper, we will assess the performance of Julia with respect to HPC. To that end, we examine the raw throughput achievable with Julia using a new Julia port of the well-known STREAM benchmark. We also compare the running times of the most commonly used MPI collective operations (e.g., MPI\_Allreduce) with their C counterparts. Our analysis shows that HPC performance of Julia is on-par with C in the majority of cases.},
  file = {/home/alfarizi/Zotero/storage/SXKVFG9D/Hunold_Steiner_2020_Benchmarking Julia’s Communication Performance.pdf;/home/alfarizi/Zotero/storage/NVP7KNBF/9307882.html}
}

@misc{intelCPUVsGPU2023,
  title = {{{CPU}} vs. {{GPU}}: {{What}}'s the {{Difference}}?},
  shorttitle = {{{CPU}} vs. {{GPU}}},
  author = {{Intel}},
  year = {2023},
  journal = {Intel},
  urldate = {2023-11-06},
  abstract = {Learn about the CPU vs GPU difference, explore uses and the architecture benefits, and their roles for accelerating deep-learning and AI.},
  howpublished = {https://www.intel.com/content/www/us/en/products/docs/processors/cpu-vs-gpu.html},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/C4SSNAF4/cpu-vs-gpu.html}
}

@article{khairySurveyArchitecturalApproaches2019,
  title = {A Survey of Architectural Approaches for Improving {{GPGPU}} Performance, Programmability and Heterogeneity},
  author = {Khairy, Mahmoud and Wassal, Amr G. and Zahran, Mohamed},
  year = {2019},
  month = may,
  journal = {Journal of Parallel and Distributed Computing},
  volume = {127},
  pages = {65--88},
  issn = {0743-7315},
  doi = {10.1016/j.jpdc.2018.11.012},
  urldate = {2023-11-06},
  abstract = {With the skyrocketing advances of process technology, the increased need to process huge amount of data, and the pivotal need for power efficiency, the usage of Graphics Processing Units (GPUs) for General Purpose Computing becomes a trend and natural. GPUs have high computational power and excellent performance per watt, for data parallel applications, relative to traditional multicore processors. GPUs appear as discrete or embedded with Central Processing Units (CPUs), leading to a scheme of heterogeneous computing. Heterogeneous computing brings as many challenges as it brings opportunities. To get the most of such systems, we need to guarantee high GPU utilization, deal with irregular control flow of some workloads, and struggle with far-friendly-programming models. The aim of this paper is to provide a survey about GPUs from two perspectives: (1) architectural advances to improve performance and programmability and (2) advances to enhance CPU{\textendash}GPU integration in heterogeneous systems. This will help researchers see the opportunities and challenges of using GPUs for general purpose computing, especially in the era of big data and the continuous need of high-performance computing.},
  keywords = {Control divergence,GPGPU,Heterogeneous architecture,Memory systems},
  file = {/home/alfarizi/Zotero/storage/X2XJHR56/S0743731518308669.html}
}

@article{kipferUberFlowGPUBasedParticle2004,
  title = {{{UberFlow}}: {{A GPU-Based Particle Engine}}},
  author = {Kipfer, Peter and Segal, Mark and Westermann, Rudiger},
  year = {2004},
  abstract = {We present a system for real-time animation of large particle sets using GPU computation and memory objects in OpenGL. Our system implements a versatile particle engine, including inter-particle collisions and visibility sorting. By combining memory objects with floating-point fragment programs, we have implemented a particle engine that entirely avoids the transfer of particle data at runtime.},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/DEHPWA7D/Kipfer et al. - UberFlow A GPU-Based Particle Engine.pdf}
}

@book{kolbHardwarebasedSimulationCollision2004,
  title = {Hardware-Based {{Simulation}} and {{Collision Detection}} for {{Large Particle Systems}}},
  author = {Kolb, A. and Latta, L. and {Rezk-Salama}, C.},
  year = {2004},
  publisher = {{The Eurographics Association}},
  issn = {1727-3471},
  doi = {10.2312/EGGH/EGGH04/123-132},
  urldate = {2023-12-10},
  abstract = {Particle systems have long been recognized as an essential building block for detail-rich and lively visual environments. Current implementations can handle up to 10,000 particles in real-time simulations and are mostly limited by the transfer of particle data from the main processor to the graphics hardware (GPU) for rendering. This paper introduces a full GPU implementation using fragment shaders of both the simulation and rendering of a dynamically-growing particle system. Such an implementation can render up to 1 million particles in real-time on recent hardware. The massively parallel simulation handles collision detection and reaction of particles with objects for arbitrary shape. The collision detection is based on depth maps that represent the outer shape of an object. The depth maps store distance values and normal vectors for collision reaction. Using a special texturebased indexing technique to represent normal vectors, standard 8-bit textures can be used to describe the complete depth map data. Alternately, several depth maps can be stored in one floating point texture. In addition, a GPU-based parallel sorting algorithm is introduced that can be used to perform a depth sorting of the particles for correct alpha blending.},
  isbn = {978-3-905673-15-9},
  langid = {english},
  annotation = {Accepted: 2013-10-28T10:02:18Z},
  file = {/home/alfarizi/Zotero/storage/Y6UAMFCL/Kolb et al_2004_Hardware-based Simulation and Collision Detection for Large Particle Systems.pdf}
}

@incollection{kukunasChapterIntelPentium2015,
  title = {Chapter 2 - {{Intel}}{\textregistered} {{Pentium}}{\textregistered} {{Processors}}},
  booktitle = {Power and {{Performance}}},
  author = {Kukunas, Jim},
  year = {2015},
  month = jan,
  pages = {31--41},
  publisher = {{Morgan Kaufmann}},
  address = {{Boston}},
  doi = {10.1016/B978-0-12-800726-6.00002-1},
  urldate = {2023-11-06},
  abstract = {This chapter builds on the previous chapter by exploring the Intel(R) Pentium(R) processor family. The chapter begins by looking at the changes introduced by the Intel(R) Pentium(R), Intel(R) Pentium(R) Pro, and Intel(R) Pentium(R) 4 processors. Topics covered include the superscalar execution pipeline, out-of-order execution, {$\mu$}ops, and Intel(R) Hyper-Threading. The chapter ends with the extension from 32- to 64-bit processors.},
  isbn = {978-0-12-800726-6},
  keywords = {Hyper-Threading,i686,Out-of-order execution,Pentium(R),Pentium(R) 4,Pentium(R) II,Pentium(R) III,Pentium(R) Pro,superscalar},
  file = {/home/alfarizi/Zotero/storage/JQUGDIPZ/B9780128007266000021.html}
}

@misc{oanceaGPGPUComputing2014,
  title = {{{GPGPU Computing}}},
  author = {Oancea, Bogdan and Andrei, Tudorel and Dragoescu, Raluca Mariana},
  year = {2014},
  month = aug,
  number = {arXiv:1408.6923},
  eprint = {1408.6923},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-12-09},
  abstract = {Since the first idea of using GPU to general purpose computing, things have evolved over the years and now there are several approaches to GPU programming. GPU computing practically began with the introduction of CUDA (Compute Unified Device Architecture) by NVIDIA and Stream by AMD. These are APIs designed by the GPU vendors to be used together with the hardware that they provide. A new emerging standard, OpenCL (Open Computing Language) tries to unify different GPU general computing API implementations and provides a framework for writing programs executed across heterogeneous platforms consisting of both CPUs and GPUs. OpenCL provides parallel computing using task-based and data-based parallelism. In this paper we will focus on the CUDA parallel computing architecture and programming model introduced by NVIDIA. We will present the benefits of the CUDA programming model. We will also compare the two main approaches, CUDA and AMD APP (STREAM) and the new framwork, OpenCL that tries to unify the GPGPU computing models.},
  archiveprefix = {arxiv},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing}},
  file = {/home/alfarizi/Zotero/storage/IRAH4M89/Oancea et al_2014_GPGPU Computing.pdf;/home/alfarizi/Zotero/storage/SFLKS67H/1408.html}
}

@article{owensSurveyGeneralPurpose2007,
  title = {A {{Survey}} of {{General}}-{{Purpose Computation}} on {{Graphics Hardware}}},
  author = {Owens, John D. and Luebke, David and Govindaraju, Naga and Harris, Mark and Kr{\"u}ger, Jens and Lefohn, Aaron E. and Purcell, Timothy J.},
  year = {2007},
  month = mar,
  journal = {Computer Graphics Forum},
  volume = {26},
  number = {1},
  pages = {80--113},
  issn = {0167-7055, 1467-8659},
  doi = {10.1111/j.1467-8659.2007.01012.x},
  urldate = {2023-12-10},
  abstract = {The rapid increase in the performance of graphics hardware, coupled with recent improvements in its programmability, have made graphics hardware a compelling platform for computationally demanding tasks in a wide variety of application domains. In this report, we describe, summarize, and analyze the latest research in mapping general-purpose computation to graphics hardware.},
  langid = {english},
  file = {/home/alfarizi/Zotero/storage/A4N5D7IQ/Owens et al. - 2007 - A Survey of General‐Purpose Computation on Graphic.pdf}
}

@article{owensSurveyGeneralPurposeComputation2007,
  title = {A {{Survey}} of {{General-Purpose Computation}} on {{Graphics Hardware}}},
  author = {Owens, John D. and Luebke, David and Govindaraju, Naga and Harris, Mark and Kr{\"u}ger, Jens and Lefohn, Aaron E. and Purcell, Timothy J.},
  year = {2007},
  journal = {Computer Graphics Forum},
  volume = {26},
  number = {1},
  pages = {80--113},
  issn = {1467-8659},
  doi = {10.1111/j.1467-8659.2007.01012.x},
  urldate = {2023-12-10},
  abstract = {The rapid increase in the performance of graphics hardware, coupled with recent improvements in its programmability, have made graphics hardware a compelling platform for computationally demanding tasks in a wide variety of application domains. In this report, we describe, summarize, and analyze the latest research in mapping general-purpose computation to graphics hardware. We begin with the technical motivations that underlie general-purpose computation on graphics processors (GPGPU) and describe the hardware and software developments that have led to the recent interest in this field. We then aim the main body of this report at two separate audiences. First, we describe the techniques used in mapping general-purpose computation to graphics hardware. We believe these techniques will be generally useful for researchers who plan to develop the next generation of GPGPU algorithms and techniques. Second, we survey and categorize the latest developments in general-purpose application development on graphics hardware.},
  langid = {english},
  keywords = {D.2.2 Software Engineering: Design tools and techniques,data-parallel computing,general-purpose computing on graphics hardware,GPGPU,GPU,graphics hardware,high-performance computing,HPC,I.3.1 Computer Graphics: Hardware architecture,I.3.6 Computer Graphics: Methodology and techniques,parallel computing,SIMD,stream computing,stream processing},
  file = {/home/alfarizi/Zotero/storage/8VIX7ADB/Owens et al_2007_A Survey of General-Purpose Computation on Graphics Hardware.pdf;/home/alfarizi/Zotero/storage/PNUIP2ZN/j.1467-8659.2007.01012.html}
}
